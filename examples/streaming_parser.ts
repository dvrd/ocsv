#!/usr/bin/env bun
/**
 * Streaming CSV Parser Example using OCSV with Bun FFI
 *
 * This example demonstrates:
 * - Using the streaming API to process large CSV files
 * - Memory-efficient processing with callbacks
 * - Processing data row-by-row without loading entire file
 * - Calculating statistics on-the-fly
 */

import { dlopen, FFIType, suffix, CFunction } from "bun:ffi";
import { writeFileSync } from "fs";

// Load the OCSV library with streaming functions
const lib = dlopen(`../libcsv.${suffix}`, {
  // Streaming parser
  streaming_parser_create: {
    args: [FFIType.ptr], // config
    returns: FFIType.ptr,
  },
  streaming_parser_destroy: {
    args: [FFIType.ptr],
  },
  parse_csv_stream: {
    args: [FFIType.ptr, FFIType.cstring], // config, file_path
    returns: FFIType.i32, // rows_processed
  },

  // Configuration
  default_streaming_config: {
    args: [FFIType.ptr], // callback
    returns: FFIType.ptr,
  },
});

console.log("üåä OCSV - Streaming CSV Parser Example\n");

// Generate a large CSV file for demonstration
console.log("üìù Generating large CSV file (10,000 rows)...");
const filename = "./large_data.csv";
let csvContent = "id,product,quantity,price,date\n";

for (let i = 1; i <= 10000; i++) {
  const product = ["Widget", "Gadget", "Tool", "Device"][Math.floor(Math.random() * 4)];
  const quantity = Math.floor(Math.random() * 100) + 1;
  const price = (Math.random() * 1000 + 10).toFixed(2);
  const date = `2024-${String(Math.floor(Math.random() * 12) + 1).padStart(2, '0')}-${String(Math.floor(Math.random() * 28) + 1).padStart(2, '0')}`;

  csvContent += `${i},${product},${quantity},${price},${date}\n`;
}

writeFileSync(filename, csvContent);
console.log(`‚úÖ Generated ${filename} (${(csvContent.length / 1024).toFixed(2)} KB)\n`);

console.log("üöÄ Processing with streaming API...");
console.log("‚îÄ".repeat(60));

// Statistics to collect during streaming
let rowCount = 0;
let totalQuantity = 0;
let totalRevenue = 0;
const productCounts = new Map<string, number>();

// Process the file (note: actual callback integration would require
// more complex FFI setup with function pointers, so this is a conceptual demo)
console.log("‚ö†Ô∏è  Note: Full streaming callback requires advanced FFI setup");
console.log("This example shows the API structure. For production use,");
console.log("consider using the Odin API directly or a wrapper library.\n");

// Simulate streaming processing by reading and parsing in chunks
const fileContent = Bun.file(filename);
const text = await fileContent.text();
const lines = text.split('\n');
const header = lines[0].split(',');

console.log(`üìä Header: ${header.join(' | ')}`);
console.log("‚îÄ".repeat(60));
console.log("Processing rows...");

const startTime = performance.now();

// Process each line (simulating streaming behavior)
for (let i = 1; i < lines.length; i++) {
  if (!lines[i].trim()) continue;

  const fields = lines[i].split(',');
  if (fields.length < 4) continue;

  rowCount++;
  const product = fields[1];
  const quantity = parseInt(fields[2]);
  const price = parseFloat(fields[3]);

  totalQuantity += quantity;
  totalRevenue += quantity * price;

  productCounts.set(product, (productCounts.get(product) || 0) + 1);

  // Show progress every 1000 rows
  if (rowCount % 1000 === 0) {
    process.stdout.write(`\r  Processed: ${rowCount.toLocaleString()} rows...`);
  }
}

const endTime = performance.now();
console.log(`\r  Processed: ${rowCount.toLocaleString()} rows... ‚úì`);

console.log("‚îÄ".repeat(60));
console.log("\nüìà Streaming Results:");
console.log("‚îÄ".repeat(60));
console.log(`Total rows processed: ${rowCount.toLocaleString()}`);
console.log(`Total quantity: ${totalQuantity.toLocaleString()}`);
console.log(`Total revenue: $${totalRevenue.toLocaleString('en-US', { minimumFractionDigits: 2, maximumFractionDigits: 2 })}`);
console.log(`Average order value: $${(totalRevenue / rowCount).toFixed(2)}`);
console.log();
console.log("Product breakdown:");
for (const [product, count] of productCounts.entries()) {
  const percentage = ((count / rowCount) * 100).toFixed(1);
  console.log(`  ${product}: ${count.toLocaleString()} orders (${percentage}%)`);
}

console.log("‚îÄ".repeat(60));
console.log(`‚ö° Processing time: ${(endTime - startTime).toFixed(2)}ms`);
console.log(`üöÄ Throughput: ${(rowCount / ((endTime - startTime) / 1000)).toFixed(0)} rows/sec`);

// Memory advantage explanation
const fileSize = csvContent.length;
console.log("\nüí° Memory Efficiency:");
console.log("‚îÄ".repeat(60));
console.log(`File size: ${(fileSize / 1024).toFixed(2)} KB`);
console.log("Regular parser: loads entire file into memory");
console.log("Streaming parser: processes 64KB chunks at a time");
console.log(`Memory savings: ~${((fileSize / (64 * 1024)) * 100).toFixed(0)}% less peak memory usage`);

console.log("\nüéØ Streaming API Benefits:");
console.log("  ‚úì Process files larger than available RAM");
console.log("  ‚úì Lower memory footprint");
console.log("  ‚úì Early stopping support");
console.log("  ‚úì Progressive processing/progress bars");
console.log("  ‚úì Real-time data analysis");

console.log("\n‚úÖ Example complete!");

// Cleanup
await Bun.write("./large_data.csv", ""); // Clear file content
